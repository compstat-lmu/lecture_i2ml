{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "plt.rcParams['figure.figsize'] = 10,10\n",
    "from utils import abline, BacktrackingLineSearch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a discriminant approach, i.e., we are modeling the posterior probabilities $\\pi(x)$ of the\n",
    "labels directly using the logistic function, s.t.\n",
    "$$\\pi(x)=P(y=1/x)=\\frac{1}{1+exp(-\\theta^Tx)}$$\n",
    "Note As in the lecture we suppress the intercept in notation, i.e. $\\theta^Tx:=\\theta_0+\\theta^Tx$. This can be easily implemented as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X,theta):\n",
    "    return 1/(1+np.exp(-X.dot(theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can observe that logistic regression “squashes” the estimated linear scores $\\theta^Tx$ to [0, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6,6,100)\n",
    "theta = 0.8\n",
    "sns.lineplot(x,logistic(x,theta))\n",
    "plt.xlabel(r\"$\\theta^Tx$\");\n",
    "plt.ylabel(r\"$\\pi(x)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of accuracy and the encoding of the labels to {−1, 1}, one can derive the Bernoulli loss (see lecture slides):\n",
    "$$L(y,f(x))=log(1+exp(-yf(x)))$$\n",
    "For logistic regression, we have $f(x) = \\theta^Tx$. This is implemented and plotted as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_bernoulli(theta,y,X):\n",
    "#     pdb.set_trace()\n",
    "    return np.log(1+np.exp(-y*X.dot(theta).reshape(-1,1))).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the loss for y = 1 and y = −1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([np.ones_like(x),x]).T\n",
    "theta = np.array([0,1])\n",
    "sns.lineplot(x,loss_bernoulli(y=1,X=X,theta=theta))\n",
    "sns.lineplot(x,loss_bernoulli(y=-1,X=X,theta=theta))\n",
    "plt.legend(['+1','-1'])\n",
    "plt.xlabel('f(x)')\n",
    "plt.ylabel('L(y,f(x))');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, in order to minimize the loss, we should predict y = 1 if\n",
    "$$\\theta^Tx \\ge 0 \\Leftrightarrow \\pi(x) = P(y=1/x) = \\frac{1}{1+exp(-\\theta^Tx)} \\ge 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_logreg(X,theta, threshold=0.5):\n",
    "    prob = logistic(X,theta)\n",
    "    return prob>threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the loss function, the emperical risk then becomes:\n",
    "$$R_{emp}=\\frac{1}{n}\\sum_{i=1}^n L(y^{(i)},f(x^{(i)}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_bernoulli(theta,y,X):\n",
    "    return np.mean(loss_bernoulli(theta,y,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression - example\n",
    "To investigate how the parameter $\\theta$ of the logistic regression can be estimated, we create some artificial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "p=2\n",
    "slope = 3.0\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.uniform(-5,5,n*p).reshape(n,p)\n",
    "theta = np.array([slope,-1])\n",
    "\n",
    "y = (2*np.random.binomial(1,logistic(X,theta))-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a logistic regression classifier from sklearn and plot the predictions after fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X, y.ravel())\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "slope_pred,intercept_pred = clf.coef_[0]\n",
    "print(slope_pred,intercept_pred)\n",
    "abline(slope_pred, intercept_pred)\n",
    "sns.scatterplot(X[:,0],X[:,1],hue=y.squeeze());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that our data is linearly separated as one would expect, since our decision boundary is defined by $\\theta^Tx=0$ which defines a hyperplane.\n",
    "\n",
    "Furthermore, we note that $\\theta$ represents a vector normal to this plane and is pointing to the “1”-side. But\n",
    "doesn’t that mean that for any positive $\\lambda \\in \\mathbb{R^+}$, a rescaled coefficient vector $\\lambda\\theta$, which defines the same\n",
    "“direction” of decision boundary as $\\theta$, would separate the data equally well. . . ?\n",
    "\n",
    "Check the classification and the empirical risk for $\\lambda \\in \\{0.5, 2, 3\\}$. Can you figure out what determines the optimal length of $\\theta$?\n",
    "\n",
    "To gain further understanding of this behavior, let’s look at a data scenario where we encounter so-called **complete separation**, i.e., a situation in which the data can be\n",
    "classified without error by our classification method. We can simply use the predictions from our model as if they were the observed labels to create such a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_compl_sep = clf.predict(X)\n",
    "thetas = np.array([0.5,1.0,2.0,10.0])*theta.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the Bernoulli loss was defined as : $L(y,f(x))=log(1+exp(-y\\theta^Tx))$\n",
    "\n",
    "In the case of complete separation, since every observation is classified correctly, hence $\\theta^Tx>0$ for y=1 and $\\theta^Tx<0$ for y=-1. Then the bernoulli loss becomes $L(y,f(x))=log(1+exp(-|\\theta^Tx|))$.\n",
    "\n",
    "This means the empirical risk is monotonously decreasing as the length of theta increases, which means that we never converge on a finite solution by minimizing the empirical risk. In this situation, additional artificial constraints – such as regularization – can be introduced to find a solution. In paractice, complete separation can happen fairly easily in datasets with a small number of observations compared to the number of available features.\n",
    "\n",
    "Since we don’t need to deal with complete separation in our example, we can simply apply the gradient_descent_opt_stepsize function of the **code demo to the linear model** directly to the empirical risk we derived from the Bernoulli loss. This type of gradient descent mimization can be applied to any risk for which we can figure out the gradient, in this case this is\n",
    "\n",
    "$$\\frac{\\partial R_{emp}(f)}{\\partial\\theta} = \\sum_{i=1}^n \\frac{-y^{(i)}}{1+exp(y^{(i)}\\theta^Tx^{(i)})}x^{(i)} $$\n",
    "\n",
    "Which can be implemented as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_bernoulli(theta,y,X):\n",
    "    loss_i = -y*X/(1+np.exp(-y*X.dot(theta).reshape(-1,1)))\n",
    "    return np.sum(loss_i,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the gradient descent method to find a numerical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_opt_stepsize(y,X,theta,\n",
    "                                  loss_fn=risk_bernoulli,\n",
    "                                  grad_fn=gradient_bernoulli,\n",
    "                                  max_iter=100,\n",
    "                                  threshold=1e-8):\n",
    "\n",
    "    loss_storage=[]\n",
    "    theta_storage=[]\n",
    "    lr_storage=[]\n",
    "    \n",
    "    loss_storage.append(loss_fn(theta,y,X))\n",
    "    theta_storage.append(theta)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        lr_opt = BacktrackingLineSearch(loss_fn,grad_fn,theta,-grad_fn(theta,y,X),args=(y,X))\n",
    "#         print(i,lr_opt)\n",
    "        theta = theta - lr_opt * grad_fn(theta,y,X)\n",
    "        loss = loss_fn(theta,y,X)\n",
    "        \n",
    "        if (i>1)&(np.sqrt(np.sum((theta_storage[-1]-theta)**2))<threshold):\n",
    "            print(f'threshold reached. Breaking at iteration : {i}')\n",
    "            break\n",
    "        loss_storage.append(loss)\n",
    "        theta_storage.append(theta)\n",
    "        lr_storage.append(lr_opt)\n",
    "        \n",
    "    return loss_storage,theta_storage,lr_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=thetas[:,0]\n",
    "print(theta)\n",
    "theta_init=np.array([-1,-1.5])\n",
    "y = (2*np.random.binomial(1,logistic(X,theta))-1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=gradient_descent_opt_stepsize(y,X,theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
