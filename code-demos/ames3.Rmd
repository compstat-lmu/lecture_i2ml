---
output: pdf_document
title: "mlr3 Usecase Day 2 - Ames Housing Dataset"
---
```{r, echo = FALSE}
knitr::opts_chunk$set(echo = solution, eval = solution)
```

This exercise is intended to use `mlr3tuning` to improve the results of the benchmark analysis on the _Ames Housing Dataset_ of day 1.

The main objective of this exercise is as follows:

- To apply an appropriate tuning technique to a learning algorithm in order to obtain improve its predictive performance.

Keep in mind that you are asked to use the Mean Squared Error (MSE) as a performance measure to evaluate the performance of a tuned algorithm.

## Accessing the dataset

The dataset is available on Kaggle https://bit.ly/2l0uWoz.
Kaggle is a platform which provides data science competitions and datasets which can be used to get familiar with typical machine learning methods.

## Importing the data

```{r}
housing = read.csv("data/ames_housing_train.csv")
```

1. Load the `mlr3` and `mlr3learners` packages.

```{r}
library(mlr3)
library(mlr3learners)
```

2. Create a regression task object.

```{r}
task = TaskRegr$new(id = "ames_housing", backend = housing, target = "SalePrice")
task
```

3. Create a list of learning algorithms which you want to use in the benchmark.

```{r}
# get a featureless learner and a regression tree
library(mlr3learners)
xgboost = lrn("regr.xgboost", nrounds = 100L)
```

```{r}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("eta", lower = 0.01, upper = 0.2),
  ParamDbl$new("gamma", lower = 2^-5, upper = 2^6),
  ParamInt$new("max_depth", lower = 3, upper = 10)
))
tune_ps
```

4. Create a resampling object for your benchmark evaluation.

```{r}
library(mlr3tuning)
# we want to use an inner 3-fold cross validation
inner_resampling = rsmp("cv", folds = 3)
terminator = term("evals", n_evals = 60L) # set terminator to 360 seconds
measure = msr("regr.mse") # we want to optimize the MSE
tuner = tnr("random_search")
```

5. Let's use the `autotuner` to use the tuned learner as a regular one within our previouse benchmark

```{r}
xgboost_tuned = AutoTuner$new(
  learner = xgboost,
  resampling = inner_resampling,
  measures = measure,
  tune_ps = tune_ps,
  terminator = terminator,
  tuner = tuner
)
xgboost_tuned
```

5. Create a grid corresponding to the planned benchmark including the task, all learners and the resampling strategy.

```{r}
# create a BenchmarkDesign object
library(mlr3pipelines)

learners = list(
  featureless = lrn("regr.featureless"),
  knn = lrn("regr.kknn"),
  knn_tuned = knn_tuned,
  tree = lrn("regr.rpart"),
  random_forest = lrn("regr.ranger"),
  xgboost = po("encode", method = "one-hot") %>>% xgboost,
  xgboost_tuned = po("encode", method = "one-hot") %>>% xgboost_tuned
)
resampling = rsmp("cv", folds = 10)
design = benchmark_grid(task, learners, resampling)
print(design)
```

6. Run the benchmark

```{r}
# execute the benchmark
bmr = benchmark(design)
```

7. Use appropriate regression measures to measure the performance of each learner in the benchmark.

```{r}
# get some measures: accuracy (acc) and area under the curve (auc)
measures = mlr_measures$mget(c("regr.mse", "regr.mae"))
bmr$aggregate(measures)
```

8. Use an appropriate plot to illustrate the benchmark results.
Have e.g. a look at the `mlr3viz` package.

```{r}
library(mlr3viz)
autoplot(bmr)
```


```{r}
test_set = read.csv("data/ames_housing_test.csv")
final_learner = learners$xgboost_tuned
final_learner$train(task)
pred = final_learner$predict_newdata(task, test_set)

# we can save the predictions as data.table and export them for Kaggle
pred = as.data.table(pred)
pred$truth = NULL
write.csv(pred, "data/ames_housing_submission_day1.csv")
```